{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DiscoBench","text":"<p>DiscoBench is a modular benchmark for automated algorithm discovery in machine learning.</p>"},{"location":"#what-is-discobench","title":"What is DiscoBench?","text":"<p>DiscoBench is a new, open-ended benchmark and research playground for developing automated algorithm discovery and AI scientist systems. DiscoBench has a modular setup, an emphasis on discovering algorithms that transfer, and a huge diversity of tasks! We hope DiscoBench helps drive the frontier of research in algorithm discovery by providing a large-scale, open-ended landscape for evaluating AI research agents!</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modular Architecture: Break down ML algorithms into composable components</li> <li>Multiple Domains: Support for reinforcement learning, language modeling, computer vision, Bayesian optimization, and more</li> <li>Flexible Configuration: Easy switching between baseline and experimental implementations</li> <li>LLM-Ready: Designed for automated algorithm discovery using AI agents</li> <li>Extensible: Simple framework for adding new tasks and domains</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Install from source:</p> <pre><code>git clone git@github.com:AlexGoldie/discobench.git\ncd discobench\nmake install\n</code></pre> <p>or install from pip:</p> <pre><code>pip install discobench\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>List available domains:</p> <pre><code>uv run discobench get-domains\n</code></pre> <p>Create a full task-domain codebase (with baseline implementations):</p> <pre><code>uv run discobench create-task --task-domain OnPolicyRL\n</code></pre> <p>Create an example task for algorithm discovery:</p> <pre><code>uv run discobench create-task --task-domain OnPolicyRL --example\n</code></pre> <p>See the full Usage Guide for detailed instructions.</p>"},{"location":"#available-domains","title":"Available Domains","text":"<p>DiscoBench currently supports the following task domains:</p> <ul> <li>OnPolicyRL: On-policy reinforcement learning (PPO-style algorithms)</li> <li>OffPolicyRL: Off-policy reinforcement learning (DQN-style algorithms)</li> <li>LanguageModelling: Pre-training language models</li> <li>ComputerVisionClassification: Image classification tasks</li> <li>BayesianOptimisation: Black-box optimization</li> <li>BrainSpeechDetection: Neural signal analysis</li> <li>ModelUnlearning: LLM unlearning tasks</li> <li>UnsupervisedEnvironmentDesign: Environment curriculum learning</li> <li>ContinualLearning: Learning under non-stationarity</li> <li>GreenhouseGasPrediction: Predicting atmospheric greenhouse gas concentrations</li> </ul> <p>See the Domains page for detailed information about each domain.</p>"},{"location":"#how-it-works","title":"How It Works","text":""},{"location":"#1-modular-components","title":"1. Modular Components","text":"<p>Each task domain is decomposed into modules. For example, OnPolicyRL includes: - <code>loss.py</code>: Objective function (e.g., PPO loss) - <code>networks.py</code>: Neural network architectures - <code>optim.py</code>: Optimization algorithms - <code>train.py</code>: Training loop logic</p>"},{"location":"#2-base-and-edit-implementations","title":"2. Base and Edit Implementations","text":"<p>Each module has two versions: - Base: Fully implemented, tested baseline - Edit: Template with function signatures for customization</p>"},{"location":"#3-configuration-driven","title":"3. Configuration-Driven","text":"<p>Control which modules use baseline vs. custom implementations via YAML config:</p> <pre><code>change_optim: true   # Use custom optimizer\nchange_loss: false   # Use baseline loss\nchange_networks: false\nchange_train: false\n</code></pre>"},{"location":"#4-task-generation","title":"4. Task Generation","text":"<p>DiscoBench assembles the configured modules into a complete, runnable task in <code>task_src/</code>:</p> <pre><code>discobench create-task --task-domain OnPolicyRL\ncd task_src/OnPolicyRL\npython run_main.py\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#for-users","title":"For Users","text":"<ul> <li>Usage Guide: CLI commands, Python API, and workflows</li> <li>Domains: Available task domains and their modules</li> </ul>"},{"location":"#for-contributors","title":"For Contributors","text":"<ul> <li>Contributing Overview: How to add new tasks to DiscoBench</li> <li>Dataset Integration: Adding new datasets to tasks</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":""},{"location":"#algorithm-discovery-with-llms","title":"Algorithm Discovery with LLMs","text":"<p>Use DiscoBench to have AI agents discover new ML algorithms: 1. Configure which modules should be generated by the LLM 2. LLM writes implementations for those modules 3. Evaluate performance across multiple tasks 4. Iterate and refine based on results</p>"},{"location":"#transfer-learning-research","title":"Transfer Learning Research","text":"<p>Test if components discovered on one task generalize to others: 1. Discover algorithm on training tasks 2. Evaluate on held-out test tasks 3. Measure generalization across domains</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>discobench/\n\u251c\u2500\u2500 tasks/              # Task domain implementations\n\u2502   \u251c\u2500\u2500 OnPolicyRL/\n\u2502   \u251c\u2500\u2500 LanguageModelling/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 utils/              # Core utilities\n\u251c\u2500\u2500 create_task.py      # Task generation logic\n\u251c\u2500\u2500 create_config.py    # Configuration utilities\n\u2514\u2500\u2500 cli.py              # Command-line interface\n\ntask_src/               # Generated task files (after running create-task)\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! DiscoBench grows stronger with more tasks and domains.</p> <ul> <li>Found a bug? Open an issue</li> <li>Want to add a task? See the Contributing Guide</li> <li>Adding datasets? Check the Dataset Integration Guide</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use DiscoBench in your research, please cite:</p> <pre><code>    @article{goldie2025discobench,\n      title={DiscoBench: An Open-Ended Benchmark For Algorithm Discovery},\n      author={Alexander D. Goldie and Zilin Wang and Adrian Hayler and Deepak Nathani and Edan Toledo and Ken Thampiratwong and Aleksandra Kalisz and Michael Beukman and Alistair Letcher and Shashank Reddy and Clarisse Wibault and Theo Wolf and Charles O'Neill and Jakob N. Foerster and Shimon Whiteson and Roberta Raileanu},\n      year={2025}\n    }\n</code></pre>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository: https://github.com/AlexGoldie/discobench</li> <li>Documentation: https://AlexGoldie.github.io/discobench</li> <li>Blog: https://alexgoldie.github.io/discobench-blog/</li> <li>PyPI Package: Coming soon</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms specified in the LICENSE file.</p>"},{"location":"domains/","title":"Task Domains","text":"<p>Here, we describe the various task domains available in DiscoBench. We expect this to continue to grow as our benchmark scales.</p>"},{"location":"domains/#bayesianoptimisation","title":"BayesianOptimisation","text":"<p>The agent must maximise randomly sampled variables using Bayesian Optimisation.</p>"},{"location":"domains/#modules","title":"Modules","text":"<p><code>acq_fn</code>, <code>acq_optimizer</code>, <code>domain</code>, <code>next_queries</code>, <code>surrogate</code>, <code>surrogate_optimizer</code></p>"},{"location":"domains/#datasets","title":"Datasets","text":"<p><code>Ackley1d</code>, <code>Ackley2d</code>, <code>Branin2d</code>, <code>Bukin2d</code>, <code>Cosine8d</code>, <code>DropWave2d</code>, <code>EggHolder2d</code>, <code>Griewank5d</code>, <code>Hartmann6d</code>, <code>HolderTable2d</code>, <code>Levy6d</code></p>"},{"location":"domains/#brainspeechdetection","title":"BrainSpeechDetection","text":"<p>The agent is tasked with training a speech detector based on brain MEG signals.</p>"},{"location":"domains/#modules_1","title":"Modules","text":"<p><code>loss</code>, <code>networks</code>, <code>optim</code></p>"},{"location":"domains/#datasets_1","title":"Datasets","text":"<p><code>LibriBrainSherlock1</code>, <code>LibriBrainSherlock2</code>, <code>LibriBrainSherlock3</code>, <code>LibriBrainSherlock4</code>, <code>LibriBrainSherlock5</code>, <code>LibriBrainSherlock6</code>, <code>LibriBrainSherlock7</code></p>"},{"location":"domains/#computervisionclassification","title":"ComputerVisionClassification","text":"<p>The agent must train an image classifier for a range of different image classification datasets, of varying difficulty.</p>"},{"location":"domains/#modules_2","title":"Modules","text":"<p><code>loss</code>, <code>networks</code>, <code>optim</code>, <code>preprocess</code></p>"},{"location":"domains/#datasets_2","title":"Datasets","text":"<p><code>CIFAR10</code>, <code>CIFAR10C</code>, <code>CIFAR10LT</code>, <code>CIFAR100</code>, <code>FashionMNIST</code>, <code>MNIST</code>, <code>OxfordFlowers</code>, <code>StanfordCars</code>, <code>TinyImageNet</code></p>"},{"location":"domains/#continuallearning","title":"ContinualLearning","text":"<p>The agent must train a model on different non-stationary continual learning tasks.</p>"},{"location":"domains/#modules_3","title":"Modules","text":"<p><code>optim</code>, <code>regularizer</code>, <code>replay</code>, <code>sampler</code>, <code>scheduler</code></p>"},{"location":"domains/#datasets_3","title":"Datasets","text":"<p><code>PermutedMNIST</code>, <code>SplitCIFAR100</code>, <code>TinyImageNetSplit</code></p>"},{"location":"domains/#greenhousegasprediction","title":"GreenhouseGasPrediction","text":"<p>The agent must train a model to predict the changing concentrations of different greenhouse gases in the atmosphere.</p>"},{"location":"domains/#modules_4","title":"Modules","text":"<p><code>data_processing</code>, <code>model</code></p>"},{"location":"domains/#datasets_4","title":"Datasets","text":"<p><code>CH4</code>, <code>CO2</code>, <code>N2O</code>, <code>SF6</code></p>"},{"location":"domains/#languagemodelling","title":"LanguageModelling","text":"<p>The agent must pre-train a language model on different small-scale pretraining datasets.</p>"},{"location":"domains/#modules_5","title":"Modules","text":"<p><code>loss</code>, <code>networks</code>, <code>optimizer</code></p>"},{"location":"domains/#datasets_5","title":"Datasets","text":"<p><code>LMFineWeb</code>, <code>OPCFineWebCode</code>, <code>OPCFineWebMath</code>, <code>TinyStories</code></p>"},{"location":"domains/#modelunlearning","title":"ModelUnlearning","text":"<p>The agent must unlearn certain behaviours of a pretrained model while maintaining others.</p>"},{"location":"domains/#modules_6","title":"Modules","text":"<p><code>loss</code></p>"},{"location":"domains/#datasets_6","title":"Datasets","text":"<p><code>muse</code>, <code>tofu</code>, <code>wmdp_cyber</code></p>"},{"location":"domains/#models","title":"Models","text":"<p><code>gemma-7b-it</code>, <code>Llama-2-7b-chat-hf</code>, <code>Llama-2-7b-hf</code>, <code>Llama-2-13b-hf</code>, <code>Llama-3.1-8b-Instruct</code>, <code>Llama-3.2-1B-Instruct</code>, <code>Llama-3.2-3B-Instruct</code>, <code>phi-1_5</code>, <code>Phi-3.5-mini-instruct</code>, <code>Qwen2.5-1.5B-Instruct</code>, <code>Qwen2.5-3B-Instruct</code>, <code>Qwen-2.5-7B-Instruct</code></p>"},{"location":"domains/#installation","title":"Installation","text":"<p>Please note, after installing the ModelUnlearning <code>requirements.txt</code>, you must install <code>flash-attn</code>. Please use:</p> <pre><code>pip install flash-attn==2.6.3 --no-build-isolation\n</code></pre>"},{"location":"domains/#offpolicyrl","title":"OffPolicyRL","text":"<p>The agent must train a value-based RL agent in game environments.</p>"},{"location":"domains/#modules_7","title":"Modules","text":"<p><code>config</code>, <code>networks</code>, <code>optim</code>, <code>policy</code>, <code>q_update</code>, <code>rb</code>, <code>train</code></p>"},{"location":"domains/#datasets_7","title":"Datasets","text":"<p><code>MinAtar/Asterix</code>, <code>MinAtar/Breakout</code>, <code>MinAtar/Freewar</code>, <code>MinAtar/SpaceInvaders</code></p>"},{"location":"domains/#onpolicyrl","title":"OnPolicyRL","text":"<p>The agent must train an on-policy RL agent in game and robotics environments.</p>"},{"location":"domains/#modules_8","title":"Modules","text":"<p><code>config</code>, <code>networks</code>, <code>optim</code>, <code>train</code></p>"},{"location":"domains/#datasets_8","title":"Datasets","text":"<p><code>Brax/Ant</code>, <code>Brax/HalfCheetag</code>, <code>Brax/Hopper</code>, <code>Brax/Humanoid</code>, <code>Brax/Pusher</code>, <code>Brax/Reacher</code>, <code>Brax/Walker2D</code>, <code>Craftax/Craftax</code>, <code>Craftax/Craftax-Classic</code>, <code>MinAtar/Asterix</code>, <code>MinAtar/Breakout</code>, <code>MinAtar/Freewar</code>, <code>MinAtar/SpaceInvaders</code></p>"},{"location":"domains/#unsupervisedenvironmentdesign","title":"UnsupervisedEnvironmentDesign","text":"<p>The agent must develop level sampling methods for an on-policy RL agent.</p>"},{"location":"domains/#modules_9","title":"Modules","text":"<p><code>sample_levels</code>, <code>train_step</code>, <code>variable_config</code></p>"},{"location":"domains/#datasets_9","title":"Datasets","text":"<p><code>Kinetix/Large</code>, <code>Kinetix/Medium</code>, <code>Kinetix/Small</code>, <code>Minigrid</code></p>"},{"location":"licenses/","title":"Licenses","text":"<p>We provide relevant references for all work in <code>discobench/tasks/&lt;task_domain&gt;/utils/_reference.txt</code>. Unless otherwise specified, all referenced work ahs been used under the MIT license.</p>"},{"location":"licenses/#brainspeechdetection","title":"BrainSpeechDetection","text":"<p>pnpl uses the BSD 3-Clause \"New\" or \"Revised\" License</p>"},{"location":"licenses/#computervisionclassification","title":"ComputerVisionClassification","text":"<p>MLGym uses the Attribution-NonCommercial 4.0 International License. HuggingFace Transformers uses the Apache License. CFIAR10LT uses the Apache2.0 License.</p>"},{"location":"licenses/#languagemodelpretraining","title":"LanguageModelPretraining","text":"<p>opc-fineweb-math-corpus uses the odc-by license. fineweb uses the odc-by license. tinystories uses the Community Data License Agreement \u2013 Sharing, Version 1.0 License.</p>"},{"location":"licenses/#offpolicyrl","title":"OffPolicyRL","text":"<p>Gymnax uses the Apache2.0 License. PureJaxRL uses the Apache2.0 License.</p>"},{"location":"licenses/#onpolicyrl","title":"OnPolicyRL","text":"<p>Gymnax uses the Apache2.0 License. PureJaxRL uses the Apache2.0 License. Brax uses the Apache2.0 License.</p>"},{"location":"licenses/#unsupervisedenvironmentdesign","title":"UnsupervisedEnvironmentDesign","text":"<p>PureJaxRL uses the Apache2.0 License. SFL uses the Apache2.0 License. Minigrid uses the Apache2.0 license.</p>"},{"location":"usage/","title":"Using DiscoBench","text":"<p>This guide covers how to use DiscoBench for algorithm discovery tasks.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>Install DiscoBench using pip (once published) or from source:</p>"},{"location":"usage/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/AlexGoldie/discobench.git\ncd discobench\nmake install\n</code></pre> <p>This will: - Create a virtual environment using uv - Install all dependencies - Set up pre-commit hooks</p>"},{"location":"usage/#quick-start","title":"Quick Start","text":""},{"location":"usage/#1-list-available-domains","title":"1. List Available Domains","text":"<p>See all available task domains:</p> <pre><code>discobench get-domains\n</code></pre>"},{"location":"usage/#2-view-modules-for-each-domain","title":"2. View Modules for Each Domain","text":"<p>See which modules are available for each domain:</p> <pre><code>discobench get-modules\n</code></pre>"},{"location":"usage/#3-create-a-task","title":"3. Create a Task","text":"<p>Create task files for a specific domain:</p> <pre><code>discobench create-task --task-domain OnPolicyRL\n</code></pre> <p>This creates a training task with default configuration. The generated files will appear in the <code>task_src/</code> directory.</p>"},{"location":"usage/#cli-reference","title":"CLI Reference","text":"<p>DiscoBench provides three main commands:</p>"},{"location":"usage/#create-task","title":"<code>create-task</code>","text":"<p>Create task source files for algorithm discovery.</p> <p>Usage:</p> <pre><code>discobench create-task --task-domain DOMAIN [OPTIONS]\n</code></pre> <p>Required Options: - <code>--task-domain TEXT</code>: The task domain to create (e.g., OnPolicyRL, LanguageModelling)</p> <p>Optional Flags: - <code>--test</code>: Create test task instead of training task - <code>--config-path PATH</code>: Path to custom task_config.yaml (defaults to built-in config) - <code>--example</code>: Create example task using prebuilt example configs</p> <p>Examples:</p> <p>Create a training task for OnPolicyRL:</p> <pre><code>discobench create-task --task-domain OnPolicyRL\n</code></pre> <p>Create a test task:</p> <pre><code>discobench create-task --task-domain OnPolicyRL --test\n</code></pre> <p>Use a custom configuration:</p> <pre><code>discobench create-task --task-domain LanguageModelling --config-path my_config.yaml\n</code></pre> <p>Use the example configuration:</p> <pre><code>discobench create-task --task-domain LanguageModelling --example\n</code></pre>"},{"location":"usage/#get-domains","title":"<code>get-domains</code>","text":"<p>List all available task domains in DiscoBench.</p> <p>Usage:</p> <pre><code>discobench get-domains\n</code></pre> <p>Output: Shows a list of all available domains like OnPolicyRL, LanguageModelling, BayesianOptimisation, etc.</p>"},{"location":"usage/#get-modules","title":"<code>get-modules</code>","text":"<p>List all available modules for each domain.</p> <p>Usage:</p> <pre><code>discobench get-modules\n</code></pre> <p>Output: Shows which modular components are available in each domain (e.g., loss, networks, optim, train).</p>"},{"location":"usage/#python-api","title":"Python API","text":"<p>You can also use DiscoBench programmatically from Python:</p>"},{"location":"usage/#creating-tasks","title":"Creating Tasks","text":"<pre><code>from discobench import create_task\n\n# Create a training task\ncreate_task(task_domain=\"OnPolicyRL\", test=False)\n\n# Create a test task with custom config\ncreate_task(\n    task_domain=\"LanguageModelling\",\n    test=True,\n    config_path=\"my_config.yaml\"\n)\n</code></pre>"},{"location":"usage/#getting-domain-information","title":"Getting Domain Information","text":"<pre><code>from discobench import get_domains, get_modules\n\n# Get list of all domains\ndomains = get_domains()\nprint(domains)\n\n# Get modules for each domain\nmodules = get_modules()\nfor domain, module_list in modules.items():\n    print(f\"{domain}: {module_list}\")\n</code></pre>"},{"location":"usage/#creating-custom-configurations","title":"Creating Custom Configurations","text":"<pre><code>from discobench import create_config\n\n# Get default config for a domain\nconfig = create_config(task_domain=\"OnPolicyRL\")\n\n# Modify the config\nconfig[\"change_optim\"] = True\nconfig[\"change_loss\"] = False\n\n# Use it to create a task\ncreate_task(\n    task_domain=\"OnPolicyRL\",\n    test=False,\n    config_dict=config\n)\n</code></pre>"},{"location":"usage/#configuration-files","title":"Configuration Files","text":"<p>Task behavior is controlled by <code>task_config.yaml</code> files. Here would be an example:</p> <pre><code>train_task_id: [MinAtar/Breakout, MinAtar/Freeway]\ntest_task_id: [MinAtar/Asterix, MinAtar/SpaceInvaders]\n\nsource_path: task_src/OnPolicyRL\ntemplate_backend: default\n\nchange_optim: true\nchange_loss: true\nchange_networks: false\nchange_train: false\n</code></pre> <p>Key Fields:</p> <ul> <li><code>train_task_id</code>: Datasets/environments for training</li> <li><code>test_task_id</code>: Datasets/environments for testing</li> <li><code>source_path</code>: Where to create the task files (default: <code>task_src/</code>)</li> <li><code>template_backend</code>: Which template variant to use (e.g., default, transformer, recurrent)</li> <li><code>change_*</code>: Set to <code>true</code> to use editable module versions, <code>false</code> for baseline implementations</li> </ul>"},{"location":"usage/#common-workflows","title":"Common Workflows","text":""},{"location":"usage/#workflow-1-running-a-default-task","title":"Workflow 1: Running a Default Task","text":"<pre><code># 1. Create the task\ndiscobench create-task --task-domain OnPolicyRL\n\n# 2. Navigate to the created task\ncd task_src/OnPolicyRL\n\n# 3. Run all task_ids in the task\n# Note: this will only run if change_*=False for all *\n# or you have completed module implementations!\npython run_main.py\n</code></pre>"},{"location":"usage/#workflow-2-using-the-example-config","title":"Workflow 2: Using the example config","text":"<pre><code># 1. Create the task\ndiscobench create-task --task-domain OnPolicyRL --example\n\n# 2. Navigate to the created task\ncd task_src/OnPolicyRL\n\n# 3. Run your agent to develop new algorithms\n\n# 4. Create the test task\ndiscobench create-task --task-domain OnPolicyRL --example --test\n\n# 5. Run evaluation\npython run_main.py\n</code></pre>"},{"location":"usage/#workflow-3-customizing-module-selection","title":"Workflow 3: Customizing Module Selection","text":"<ol> <li> <p>Get the default config:    <code>python    from discobench import create_config    config = create_config(\"OnPolicyRL\")</code></p> </li> <li> <p>Modify which modules are editable:    <code>python    config[\"change_optim\"] = True  # Use editable optimizer    config[\"change_loss\"] = True   # Use editable loss</code></p> </li> <li> <p>Create task with custom config:    <code>python    from discobench import create_task    create_task(\"OnPolicyRL\", test=False, config_dict=config)</code></p> </li> </ol>"},{"location":"usage/#workflow-4-testing-across-multiple-domains","title":"Workflow 4: Testing Across Multiple Domains","text":"<pre><code># Create tasks for different domains\ndiscobench create-task --task-domain OnPolicyRL\ndiscobench create-task --task-domain LanguageModelling\ndiscobench create-task --task-domain BayesianOptimisation\n\n# Each creates files in task_src/{domain_specific_folder}/\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>See Domains for detailed information about available task domains</li> <li>See Contributing Guide to add your own tasks</li> <li>See Dataset Integration to add new datasets</li> </ul>"},{"location":"how_to/dataset_integration/","title":"\ud83d\uddc2\ufe0f Dataset Integration Guide","text":"<p>This guide explains how to add new datasets to DiscoBench tasks. This is particularly useful for task like ComputerVision or LanguageModelling, which use datasets instead of environments. We'll use FashionMNIST as our primary example throughout this guide.</p>"},{"location":"how_to/dataset_integration/#overview","title":"\ud83c\udfaf Overview","text":"<p>Datasets in DiscoBench serve two main purposes: 1. Data Download: Provide raw data for training/evaluation 2. Data Loading: Preprocess and format data for PyTorch models</p> <p>Each dataset must implement specific functions that integrate with the DiscoBench framework.</p>"},{"location":"how_to/dataset_integration/#dataset-structure","title":"\ud83d\udcc1 Dataset Structure","text":"<p>Every dataset lives in its own folder under <code>discobench/tasks/{TASK_DOMAIN}/datasets/{DATASET_NAME}/</code>:</p> <pre><code>FashionMNIST/\n\u251c\u2500\u2500 make_dataset.py    # Required: download_dataset() and load_dataset()\n\u251c\u2500\u2500 config.py          # Optional: dataset-specific configurations\n\u2514\u2500\u2500 description.md     # Required: human-readable description\n</code></pre>"},{"location":"how_to/dataset_integration/#required-functions","title":"\ud83d\udd27 Required Functions","text":""},{"location":"how_to/dataset_integration/#download_datasetdest_loc-str","title":"<code>download_dataset(dest_loc: str)</code>","text":"<p>Downloads and saves the raw dataset to the specified location.</p> <p>FashionMNIST Example:</p> <pre><code>import datasets\n\ndef download_dataset(dest_loc: str):\n    ds_dict = datasets.load_dataset(\"zalando-datasets/fashion_mnist\")\n    ds_dict.save_to_disk(dest_loc)\n</code></pre> <p>Key Points: - Called automatically by <code>make_files.py</code> during task setup for each dataset - Destination path: <code>dest_loc</code> is <code>task_src/{task_id}/data</code> where <code>task_id</code> is your dataset name (e.g., <code>task_src/FashionMNIST/data</code>) - Format: Saves data in HuggingFace datasets format (<code>.arrow</code> files) for efficient loading - Caching behavior: The first time a dataset is downloaded, it will be cached in .cache. - Integration timing: Called once per dataset during the <code>make_files()</code> process, before training code is assembled</p>"},{"location":"how_to/dataset_integration/#load_datasetsrc_loc-str-data","title":"<code>load_dataset(src_loc: str = \"./data\")</code>","text":"<p>Loads, preprocesses, and returns PyTorch-compatible datasets.</p> <p>Key Concept: <code>load_dataset()</code> serves as the interface between your specific dataset implementation and the general task code. While you can implement it however you want, it must provide a consistent interface that the task's training and evaluation code can rely on.</p> <p>Key Points: - Interface Role: Adapts raw downloaded data to the format expected by the specific task domain - Task-Specific: Different tasks expect different data formats (HuggingFace DatasetDict for CV, custom DataLoader for Language Modeling, etc.) - Consistency: The same function name <code>load_dataset()</code> is used across datasets, but implementations vary by task requirements</p>"},{"location":"how_to/dataset_integration/#example-fashionmnist-for-computer-vision-classification","title":"Example: FashionMNIST for Computer Vision Classification","text":"<p>For Computer Vision Classification tasks, the training code expects HuggingFace DatasetDict format:</p> <pre><code>import datasets\nimport torch\nimport numpy as np\n\ndef load_dataset(src_loc: str = \"./FashionMNIST/data\"):\n    # Load the raw downloaded data\n    dataset = datasets.load_from_disk(src_loc)\n\n    def preprocess_function(examples):\n        # Convert to the expected format for CV classification\n        images = torch.tensor(np.array(examples['image'])[:, None, :, :], dtype=torch.float32) / 255.0\n        return {\"pixel_values\": images, \"label\": examples[\"label\"]}\n\n    processed_datasets = dataset.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=dataset[\"train\"].column_names,\n    )\n\n    processed_datasets.set_format(\"torch\")\n    return processed_datasets\n</code></pre> <p>What this provides: - Returns a dictionary with <code>\"train\"</code> and <code>\"test\"</code> keys - Each split contains datasets compatible with HuggingFace Trainer - Standardized column names that the task code expects - PyTorch tensors ready for training</p> <p>Interface Requirements for CV Classification: - <code>load_dataset()</code> \u2192 DatasetDict with <code>[\"train\"]</code> and <code>[\"test\"]</code> splits - Each dataset has <code>\"pixel_values\"</code> (images) and <code>\"label\"</code> (targets) columns - Data is in PyTorch tensor format</p>"},{"location":"how_to/dataset_integration/#what-to-put-into-download_dataset-vs-load_dataset","title":"What to put into <code>download_dataset</code> vs. <code>load_dataset</code>:","text":"<p>Given that the output of <code>download_dataset</code> will be cached in the forseeable future, I would recommend moving any expensive overhead (if possible) into <code>download_dataset</code>. For example, for the Language Modelling task, <code>load_dataset</code> not only handles the downloading, but also the preprocessing into <code>*.bin</code> files, which is quite an expensive task in itself.</p>"},{"location":"how_to/dataset_integration/#i-think-this-open-to-debate-as-this-means-that-the-data","title":"(I think this open to debate, as this means that the data)","text":""},{"location":"how_to/dataset_integration/#integration-with-discobench","title":"\ud83c\udfa8 Integration with DiscoBench","text":""},{"location":"how_to/dataset_integration/#how-data-flows-through-the-system","title":"How Data Flows Through the System","text":"<ol> <li> <p>Task Creation (<code>make_files.py</code>):    <code>python    # For each dataset in task_config, automatically calls:    dest_loc = Path(\"task_src\") / dataset_name  # e.g., \"task_src/FashionMNIST\"    download_dataset(dest_loc / \"data\")  # Downloads to \"task_src/FashionMNIST/data\"</code></p> </li> <li> <p>Training (<code>train.py</code>):    ```python    from make_dataset import load_dataset</p> </li> </ol> <p>processed_datasets = load_dataset()    train_dataset = processed_datasets[\"train\"]    test_dataset = processed_datasets[\"test\"]    ```</p> <ol> <li>Evaluation (<code>evaluate.py</code>):    <code>python    # Currently loads test data directly for validation    dataset = load_dataset(\"zalando-datasets/fashion_mnist\", split=\"test\")</code></li> </ol>"},{"location":"how_to/overview/","title":"\ud83e\udea9 How to Contribute a Task for DiscoBench","text":"<p>Thank you for your interest in making a task for DiscoBench! Your contribution is hugely appreciated and will help unlock new research in automated research and algorithm discovery using agentic LLMs.</p>"},{"location":"how_to/overview/#goal","title":"\ud83c\udfaf Goal","text":"<p>The goal of DiscoBench is to develop a series of modular tasks, where an ML codebase is broken into its constituent components, for LLMs to use when discovering new algorithms. Through configs, we can choose which modules should use default code (the original implementation) and which should be LLM-generated. We want to ensure that LLMs can produce performant, generalisable algorithms for AI research.</p>"},{"location":"how_to/overview/#getting-started","title":"\u2699\ufe0f Getting Started","text":"<ol> <li>Follow the setup instructions from the DiscoBench repository to prepare your environment.</li> <li>Clone the repo and ensure everything runs correctly.</li> <li>Follow the guide below to create your own task.</li> </ol>"},{"location":"how_to/overview/#directory-structure-example","title":"\ud83d\udcc1 Directory Structure Example","text":"<p>Here, we will use OnPolicyRL as an example task structure. The OnPolicyRL directory looks as follows.</p> <pre><code>OnPolicyRL/\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 Brax/\n\u2502   \u251c\u2500\u2500 Craftax/\n\u2502   \u251c\u2500\u2500 GridWorld/\n\u2502   \u2514\u2500\u2500 MinAtar/\n\u2502\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 default/\n\u2502   \u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u251c\u2500\u2500 edit/\n\u2502   \u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 wrappers.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 recurrent/\n\u2502   \u251c\u2500\u2500 transformer/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 _reference.txt\n\u2502       \u251c\u2500\u2500 description.md\n\u2502       \u251c\u2500\u2500 requirements.txt\n\u2502       \u251c\u2500\u2500 task_information.yaml\n\u2502       \u2514\u2500\u2500 task_spec.yaml\n\u2502\n\u2514\u2500\u2500 task_config.yaml\n</code></pre>"},{"location":"how_to/overview/#step-by-step-explanation","title":"\ud83e\udde9 Step-by-Step Explanation","text":""},{"location":"how_to/overview/#datasets","title":"\ud83e\udde0 <code>datasets/</code>","text":"<p>Contains each dataset (or environment) that your code can run with.</p> <p>Each dataset folder should include:</p> <ul> <li><code>description.md</code> \u2014 explains what the dataset/environment is (e.g., \u201cThis is Breakout!\u201d).</li> <li><code>make_env.py</code> / <code>make_dataset.py</code> \u2014 loads and returns the dataset or environment. See <code>dataset_integration.md</code> for a more thorough explanation of how to handle datasets in your new DiscoBench task!</li> <li>Any dataset-specific configs or helper files.</li> </ul>"},{"location":"how_to/overview/#templates","title":"\ud83c\udfd7\ufe0f <code>templates/</code>","text":"<p>The <code>templates/</code> directory contains all versions of your code templates.</p>"},{"location":"how_to/overview/#must-contain","title":"Must contain:","text":"<ul> <li> <p><code>default/</code> \u2014 includes:</p> </li> <li> <p><code>base/</code>: fully implemented modules.</p> </li> <li><code>edit/</code>: same file names as <code>base/</code>, but with function signatures, comments, and possibly some useful lines only. These are the files to be completed by an LLM.</li> <li><code>main.py</code>: the main entry point to the task.</li> <li>Other necessary files like <code>wrappers.py</code>, or any model evaluation logic. Any non-modules should be stored outside of <code>base</code>/<code>edit</code>.</li> <li><code>utils/</code> \u2014 meta-information and configuration files.</li> </ul>"},{"location":"how_to/overview/#example","title":"Example","text":"<ul> <li><code>base/optim.py</code>:</li> </ul> <p> * <code>edit/optim.py</code>:</p> <p></p> <p>\ud83e\udde9 Files in <code>templates/</code> (outside <code>base</code>/<code>edit</code>) are shared \u2014 used regardless of which version (default or LLM-generated) is selected.</p>"},{"location":"how_to/overview/#utils","title":"\ud83e\uddf0 <code>utils/</code>","text":"<p>This folder always contains:</p> <ul> <li> <p><code>description.md</code> \u2014 general task-domain description (e.g., what RL is).</p> </li> <li> <p><code>requirements.txt</code> \u2014 dependencies required to run the benchmark.</p> </li> <li> <p><code>task_information.yaml</code> \u2014 describes per-module prompts for <code>edit</code> codebases.   Each <code>{module}_prompt</code> must match the corresponding filename.</p> </li> <li> <p><code>task_spec.yaml</code> \u2014 defines all files which need to be loaded to define a task. Also sets which files are fixed and which are modular.</p> </li> </ul>"},{"location":"how_to/overview/#template_backends","title":"\ud83e\udde0 <code>template_backends/</code>","text":"<p>Folders like <code>transformer/</code> or <code>recurrent/</code> are optional backends that override specific files in <code>default/</code>.</p> <p>Example:</p> <ul> <li><code>transformer/networks.py</code> replaces <code>default/networks.py</code> with a transformer implementation.</li> <li>If implementing any additional backends, there should be an updated <code>task_information.yaml</code> in the backend folder for whichever modules have been overwritten.</li> </ul>"},{"location":"how_to/overview/#task_configyaml","title":"\ud83e\uddfe <code>task_config.yaml</code>","text":"<p>Defines which modules use base or edit code. This is what anyone running the benchmark can use to configure the task.</p> <p>It also:</p> <ul> <li>Specifies the dataset/environment</li> <li>Chooses backend (default/recurrent/transformer)</li> <li>Defines where to save the task under <code>task_src/</code></li> </ul> <p><code>task_spec.yaml</code> vs <code>task_config.yaml</code>:</p> <ul> <li> <p><code>task_spec.yaml</code> (in <code>utils/</code>): Defines the structure of your task domain. It lists which files are fixed (always copied as-is) vs which are module files (can have <code>base/</code> and <code>edit/</code> versions). This file is static and defines the task domain architecture.</p> </li> <li> <p><code>task_config.yaml</code> (in task root): Defines the runtime configuration for a specific task instance. It specifies:</p> <ul> <li>Which datasets to use (<code>train_task_id</code>, <code>test_task_id</code>)</li> <li>Which modules should use <code>edit/</code> implementations (<code>change_loss: true</code>, <code>change_optim: false</code>, etc.)</li> <li>Which backend to create the task with.</li> <li>Any task-specific settings</li> </ul> </li> </ul> <p>This file is dynamic and can be modified to change which parts of the code are editable for participants.</p>"},{"location":"how_to/overview/#optional-models","title":"\ud83e\udd16 (Optional) <code>models/</code>","text":"<p>Contains different pretrained models that your code relies on. These can optionally be included in your tasks if they involve finetuning or changing pretrained models.</p> <p>Each model folder should include: * <code>description.md</code> - an explanation of that model * <code>model_config.yaml</code> - everything needed to download the model from HuggingFace.</p> <p>See <code>discobench/tasks/ModelUnlearning</code> for an example of how <code>models</code> can be used!</p>"},{"location":"how_to/overview/#how-to-make-a-new-task","title":"\ud83e\uddf1 How to Make a New Task","text":"<ol> <li> <p>Choose a codebase</p> </li> <li> <p>Stay close to a known repo for verification and reproducibility.</p> </li> <li> <p>Example: OnPolicyRL is derived from PureJaxRL.</p> </li> <li> <p>Identify modules</p> </li> <li> <p>Generally, there are some easy modules to identify: <code>network</code>, <code>loss</code>, <code>optimizer</code></p> </li> <li> <p>Optionally include <code>config</code>, <code>training loop</code>, or other unique artifacts.</p> </li> <li> <p>Split code into modules</p> </li> <li> <p>Each module should ideally have a single purpose (e.g. <code>get_optimizer()</code>).</p> </li> <li> <p>Create base and edit folders</p> </li> <li> <p><code>base/</code>: complete implementations.</p> </li> <li> <p><code>edit/</code>: empty or commented versions, keeping function signatures and minimal guidance.</p> </li> <li> <p>Define a metric</p> </li> <li> <p>Must return or print a performance metric.</p> </li> <li>E.g., validation accuracy, test score after tuning, etc.</li> <li>The logic for producing this metric must not reside in a module (otherwise the LLM could cheat)!</li> <li> <p>Be consistent across tasks!</p> </li> <li> <p>Create <code>task_spec.yaml</code></p> </li> <li> <p>List all modules and mark whether they're editable or fixed. This file defines the structure of your task and does not change. It lives in <code>utils/task_spec.yaml</code>. Below you can find an example <code>task_spec.yaml</code> file:</p> <p>```yaml    fixed_files:  - train.py  - evaluate.py  - make_dataset.py  - config.py  - main.py</p> </li> </ol> <p>module_files:      - loss.py      - networks.py      - optim.py    ```</p> <ol> <li> <p>(Optional) Add backends (<code>transformer/</code>, <code>recurrent/</code>, etc.)</p> </li> <li> <p>Write metadata</p> </li> <li> <p>Add <code>description.md</code>, <code>task_information.yaml</code>, <code>requirements.txt</code> inside <code>utils/</code>.</p> </li> <li> <p>Add datasets</p> </li> <li> <p>Each under <code>datasets/</code>, with its own <code>description.md</code> and loader/configs.</p> </li> <li> <p>Verify your code</p> <ul> <li>Ensure base code runs to expected performance.</li> <li>Check <code>edit</code> code has correct signatures and structure.</li> <li>You can temporarily replace edit with base code to verify functionality.</li> </ul> </li> <li> <p>Add <code>_reference.txt</code></p> <ul> <li>Include original codebase and dataset citation or source link.</li> </ul> </li> <li> <p>Ensure <code>main.py</code> exists</p> <ul> <li>This must be the entrypoint.</li> </ul> </li> <li> <p>Create <code>task_config.yaml</code></p> <ul> <li>This file lives in the task root directory (same level as <code>utils/</code>, <code>templates/</code>, etc.).</li> <li>It specifies which datasets to use and which modules should use <code>edit/</code> implementations.</li> <li>For every file listed in <code>module_files</code> in your <code>task_spec.yaml</code>, you must include a corresponding <code>change_&lt;module_name&gt;</code> entry (without the <code>.py</code> extension).</li> </ul> <p>Example from OnPolicyRL:</p> <p>```yaml train_task_id: [MinAtar/Breakout, MinAtar/Freeway] test_task_id: [MinAtar/Breakout, MinAtar/SpaceInvaders, MinAtar/Freeway, MinAtar/Asterix, Brax/Ant]</p> <p>source_path: task_src/OnPolicyRL template_backend: default # default, transformer, recurrent</p> <p>change_optim: false change_loss: true change_networks: false change_train: false ```</p> <ul> <li><code>train_task_id</code> and <code>test_task_id</code>: Specify which datasets to use (must match dataset folder names under <code>datasets/</code>).</li> <li><code>change_&lt;module&gt;</code>: Set to <code>true</code> to use the <code>edit/</code> version (participants can modify), <code>false</code> to use the <code>base/</code> version (fixed implementation).</li> <li>Each module file from <code>task_spec.yaml</code>'s <code>module_files</code> list needs a corresponding <code>change_&lt;module&gt;</code> entry (e.g., <code>loss.py</code> \u2192 <code>change_loss</code>, <code>networks.py</code> \u2192 <code>change_networks</code>, <code>optim.py</code> \u2192 <code>change_optim</code>).</li> </ul> </li> <li> <p>Create example_config in <code>example_configs/&lt;task_domain&gt;.yaml</code></p> <ul> <li>This will create an example task for anyone who wants to test an agent on your task.</li> </ul> <p>Example from OnPolicyRL:</p> <p>```yaml train_task_id: [MinAtar/Breakout, MinAtar/Freeway,] test_task_id: [MinAtar/Asterix, MinAtar/SpaceInvaders]</p> <p>source_path: task_src/OnPolicyRL template_backend: default # default, transformer, recurrent</p> <p>change_optim: true change_loss: true change_networks: false change_train: false ```</p> </li> <li> <p>Keep metrics outside modules</p> <ul> <li>The main performance metric should not be computed inside a module (we don't want it to be possible to cheat)!</li> </ul> </li> </ol> <p>\u2705 Done! Your task is ready for integration.</p>"},{"location":"how_to/overview/#dataset-integration","title":"\ud83d\uddc2\ufe0f Dataset Integration","text":"<p>For detailed instructions on adding new datasets to your tasks, see our Dataset Integration Guide.</p>"},{"location":"how_to/overview/#verifying-your-task","title":"\ud83e\uddea Verifying Your Task","text":"<ol> <li>Generate the LLM-facing file system</li> </ol> <p>To test whether your task is runnable, try creating the file system as it would be used in <code>discobench</code> with the command:</p> <p><code>bash    python3 -m discobench.create_task --task_domain &lt;TASK_NAME&gt;</code></p> <p>This will populate:</p> <p><code>task_src/&lt;TASK_NAME&gt;</code></p> <p>The first check should therefore be that the above runs through without any errors.</p> <ol> <li> <p>Verifying that your code can run.</p> <p>After you verified that your task can run using <code>make_files.py</code>, it is now time to actually run your code. There are many ways to do so. One easy way is to (i) change edit to <code>false</code> for all modules and (ii) include all datasets as train tasks in the <code>task_config.yaml</code>. Then re-run the script in (1); you should be able to run the files in the file system created under <code>task_src/</code>. To test this, use <code>run_main.py</code>, which will run all files called <code>main.py</code>.</p> </li> <li> <p>Make sure that all additional files are there</p> </li> </ol> <p>There are some files that are needed to generate the LLM Agent prompts, which currently do not lead to errors in steps (1) and (2), even when they are missing. While they were already mentioned in the text above, here you can find a compact collection to make sure that all the files you need are there:</p> <ul> <li><code>description.md</code> \u2014 general task-domain description (e.g., what RL is).</li> <li><code>requirements.txt</code> \u2014 dependencies required to run the benchmark.</li> <li><code>task_information.yaml</code> \u2014 describes per-module prompts for <code>edit</code> codebases.   Each <code>{module}_prompt</code> must match the corresponding filename.</li> <li><code>_reference.txt</code> \u2014 original codebase citation or source link for attribution and reproducibility.</li> <li><code>datasets/&lt;DATASET_NAME&gt;/description.md</code> \u2014 Must be provided for each dataset. Explains what the dataset/environment is (e.g., \"This is Breakout!\").</li> </ul>"},{"location":"how_to/overview/#nice-to-know","title":"\ud83d\udca1 Nice to Know","text":"<ul> <li>Running pre-commit hooks on every commit can be annoying.   You can disable them temporarily:</li> </ul> <p><code>bash   git commit --no-verify</code></p> <p>Then, when you\u2019re ready to push:</p> <p><code>bash   pre-commit run --all-files</code></p> <p>or simply commit again without <code>--no-verify</code>.</p>"},{"location":"how_to/overview/#summary","title":"\ud83e\udded Summary","text":"<p>Creating a DiscoBench task involves:</p> <ol> <li>Structuring your files (<code>datasets</code>, <code>templates</code>, <code>utils</code>).</li> <li>Separating full (<code>base</code>) and empty (<code>edit</code>) implementations.</li> <li>Adding metadata (<code>task_information.yaml</code>, <code>task_spec.yaml</code>).</li> <li>Ensuring reproducibility and attribution.</li> <li>Verifying your task with the creation script.</li> </ol> <p>Follow this guide carefully \u2014 doing so makes our lives much easier when integrating your task! \u2728</p>"}]}