Machine unlearning for large language models (LLMs) is a technique focused on selectively removing the influence of specific training data from a model's learned representations, without requiring complete retraining. Unlike traditional model updating approaches that involve retraining from scratch, unlearning methods efficiently modify model weights or behaviors to "forget" targeted information—such as sensitive personal data, copyrighted content, or harmful knowledge—while preserving the model's overall capabilities and performance on unrelated tasks.

The need for unlearning has emerged from practical concerns around data privacy, safety, and regulatory compliance. LLMs trained on massive datasets often inadvertently memorize sensitive information, copyrighted material, or harmful content. With ever-increasing costs of pre-training and post-training (often involving billions of tokens and substantial computational resources), retraining models from scratch in response to data deletion requests is prohibitively expensive. This has motivated the development of efficient post-training interventions that can selectively eliminate undesirable knowledge while maintaining model utility.

The objective of LLM unlearning is twofold: (1) **Removal** - ensuring that the influence caused specifically by the "forget set" (the data to be removed) is substantially erased from the model, and (2) **Retention** - maintaining the model's utility and performance on unrelated downstream tasks and general capabilities. Formally, given an original model trained on data containing a forget set, the unlearning process yields an unlearned model where the forget set's influence is eliminated. The efficacy is assessed using evaluation metrics that quantify remaining influence (such as extraction strength, membership inference attacks, and truth ratio) while utility metrics ensure preserved performance on general tasks.

Unlearning methods vary in their approach. While some are prompting-based, detecting sensitive queries at inference time and deploying obfuscation mechanisms, the most practical and scalable approaches directly modify model weights. These weight-modification techniques include: fine-tuning with tailored loss functions that encourage forgetting on the forget set while maintaining performance on a retain set; preference optimization methods that steer the model away from generating content related to the forget set; and representation-level interventions that modify internal activations. Success in unlearning requires careful balancing between thorough forgetting (avoiding any residual knowledge) and maintaining utility (preventing degradation of general capabilities).

Below, we provide descriptions of the unlearning datasets and benchmarks which you will be working with. However, be aware that any methods you develop should ideally generalize to other unlearning scenarios and benchmarks as well.
