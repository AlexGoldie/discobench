sample_levels_prompt: |-
  You should change the `sample_and_score_fn` function, which is defined inside the `make_sample_and_score_fn` factory in the file `sample_levels.py`. In Unsupervised Environment Design (UED), the goal is to automatically generate a curriculum of environments to train a robust agent. This function is a core component of that process. It is responsible for sampling a batch of potential training levels, evaluating the current agent's performance on them, and then assigning a "score" to each level. Levels with higher scores are more likely to be selected for the agent's subsequent training. Your task is to **replace the placeholder scoring logic**. Currently, it uses a naive implementation that scores all levels equally: `level_scores = jnp.ones_like(returns)`. You must implement a more meaningful scoring function that identifies levels that are most beneficial for training the agent. For example, you might want to score levels based on the agent's uncertainty, the variance of returns, or whether the agent is making progress (but not finding it too easy or too hard). You should not change the name or interface of the `sample_and_score_fn` function. You must ensure that it returns the same data structure: a tuple containing the `train_state` and another tuple of `(env_instances, level_scores, extra_dict)`. Note that you have access to the full trajectory data (`traj_batch`) from the agent's rollout, which includes rewards, values, dones, and other information that can be used to compute your scores.

variable_config_prompt: |-
  You should change the `variable_config` file, which is defined in `variable_config.py`. This file includes a number of important hyperparameters for reinforcement learning and unsupervised environment design (UED). You should consider the effect of each of these hyperparameters and change them to maximise the performance of the agent.

train_step_prompt: |-
  You should change the `train_step` function, which is defined in `train_step.py`. The train_step function is responsible for collecting trajectories from the environment, updating the policy based on this data, and sampling new levels for the next step of training. You should replace any placeholder logic in the existing train_step function using your own implementation. You should ensure that your function takes and returns the same data structures as the provided template.
