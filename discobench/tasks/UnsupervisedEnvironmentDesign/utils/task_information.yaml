sample_levels_prompt: |-
  You should change the `sample_and_score_fn` function, which is defined inside the `make_sample_and_score_fn` factory in the file `sample_levels.py`. In Unsupervised Environment Design (UED), the goal is to automatically generate a curriculum of environments to train a robust agent. This function is a core component of that process. It is responsible for sampling a batch of potential training levels, evaluating the current agent's performance on them, and then assigning a "score" to each level. Levels with higher scores are more likely to be selected for the agent's subsequent training. Your task is to **replace the placeholder scoring logic**. Currently, it uses a naive implementation that scores all levels equally: `level_scores = jnp.ones_like(returns)`. You must implement a more meaningful scoring function that identifies levels that are most beneficial for training the agent. For example, you might want to score levels based on the agent's uncertainty, the variance of returns, or whether the agent is making progress (but not finding it too easy or too hard). You should not change the name or interface of the `sample_and_score_fn` function. You must ensure that it returns the same data structure: a tuple containing the `train_state` and another tuple of `(env_instances, level_scores, extra_dict)`. Note that you have access to the full trajectory data (`traj_batch`) from the agent's rollout, which includes rewards, values, dones, and other information that can be used to compute your scores.
