Multi-agent reinforcement learning (MARL) extends the RL framework to settings where multiple agents simultaneously interact with a shared environment. Each agent observes (part of) the state, selects actions according to its own policy, and receives reward signals that may depend on the joint behaviour of all agents. The core challenge is that the environment is non-stationary from any single agent's perspective: as all agents learn and adapt simultaneously, the dynamics each agent experiences keep changing.

Historically, MARL draws on both classical game theory—which studies strategic interaction between rational agents—and single-agent RL. Early formalizations modelled multi-agent problems as stochastic games (also called Markov games), a generalisation of Markov decision processes to multiple players. The cooperative setting, where agents share a common reward and must coordinate to maximise it, is particularly relevant to robotic control and other physical systems.

The objective in cooperative MARL is to find a joint policy—a collection of per-agent policies—that maximises the expected cumulative team reward. A common execution paradigm is centralised training with decentralised execution (CTDE): during training, agents may share global information (full state, other agents' observations or actions), but at execution time each agent acts using only its own local observation. This is the paradigm used here.

Key challenges specific to MARL include: the credit assignment problem (attributing shared reward to individual agent contributions), partial observability (each agent sees only a local view of the global state), and scalability (the joint action space grows exponentially with the number of agents). Understanding how observations and actions are partitioned across agents—which joints or body parts each agent controls, and what subset of the global state it observes—is therefore essential before implementing a MARL algorithm.

Below, we provide a description of the environment which you will be training in. However, be aware that any code you develop may be applied to other MARL environments too.
