Reinforcement learning is a branch of machine learning focused on training agents to make sequences of decisions in an environment to maximize a notion of cumulative reward. Unlike supervised learning, where models learn from labeled examples, RL agents learn through trial and error, receiving feedback in the form of rewards or penalties based on their actions. The core idea is that the agent explores the environment, evaluates the outcomes of its actions, and gradually improves its decision-making policy to achieve better long-term results.

Historically, RL draws inspiration from behavioral psychology, particularly the study of how animals learn from rewards and punishments. Early formalizations in the 1950s and 1960s laid the groundwork for algorithms that could handle Markov decision processes (MDPs), a mathematical framework for modeling decision-making under uncertainty. The agent-environment interaction loop—where the agent observes the state of the environment, takes an action, and receives a reward—remains central to all RL formulations.

The objective of reinforcement learning is to find a policy—a mapping from states to actions—that maximizes the expected cumulative reward over time. RL algorithms vary in approach, from value-based methods, which estimate the expected reward of actions, to policy-based methods, which directly optimize the agent’s behavior. Success in RL requires balancing exploration (trying new actions to discover rewards) and exploitation (leveraging known actions that yield high rewards).

In practice, RL has been applied to robotics, game playing, resource management, and recommendation systems, among other areas, where sequential decision-making is key. Understanding the principles of reward, policy, and environment dynamics is essential before implementing an RL algorithm, as these components shape how the agent learns and adapts.

You will be assisting in developing algorithms for Value-based Reinforcement Learning, which focuses on estimating the expected return (value) of states or state–action pairs under a policy. The agent learns a value function either the state-value function V(s) or the action-value function Q(s, a)—and derives an optimal policy by selecting actions that maximize these estimates. Existing algorithms iteratively update value estimates using the Bellman equation, enabling the agent to approximate the optimal policy without explicitly modeling the environment’s dynamics.

Below, we provide a description of the environment which you will be training in. However, be aware that any code you develop may be applied to other RL environments too.
