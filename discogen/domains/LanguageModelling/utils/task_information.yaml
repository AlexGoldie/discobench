optim_prompt: |-
  You should change the optimizer file, which can be found in `optim.py`. In deep learning, optimization is used to descend the gradient of a loss function with respect to the parameters of a neural network. Your task is to implement the `OptimizerConfig` dataclass with appropriate hyperparameters and the `create_optimizers` function. This function should create and return a list of optimizers and a list of learning rate schedulers for training the language model. You may use different optimizers for different parts of the model (e.g., embedding layers, transformer blocks, output head) if desired. The function receives the model and config as inputs and must return two lists: `optimizers` and `schedulers`.

loss_prompt: |-
  You should change the loss file, which can be found in `loss.py`. In language modeling, the loss provides an objective to minimize for next token prediction. Your task is to implement the `compute_loss` function that takes model logits of shape [batch_size, seq_len, vocab_size] and target tokens of shape [batch_size, seq_len], and returns a scalar loss value. Make sure to ignore padding tokens (where target is -1). You should not change the name of the function `compute_loss` or its input/output signature.

networks_prompt: |-
  You should change the network file, which can be found in `networks.py`. In deep learning, the network provides the architecture of the model and specifies the number of parameters, layer dimensions, and types of connections. Your task is to implement the `ModelConfig` dataclass with model hyperparameters and the `Model` class. The Model must have a `forward` method that takes input token indices of shape [batch_size, seq_len] and returns logits of shape [batch_size, seq_len, vocab_size]. The Model must also have a `get_config` method that returns the model configuration. You should not change the name or interface of these methods.
