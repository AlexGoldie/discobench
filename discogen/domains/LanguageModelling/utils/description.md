# Language Modelling

Language modelling is a fundamental task in natural language processing that involves predicting the next token in a sequence of text. This task forms the foundation for many modern large language models (LLMs) and serves as a key benchmark for understanding how well neural networks can learn patterns in sequential data.

In this task, you will work with autoregressive transformer models trained on text corpora. The goal is to minimize perplexity (or equivalently, cross-entropy loss) on held-out validation data. The task provides opportunities to experiment with different architectures, optimization strategies, and loss functions to improve language model performance.
